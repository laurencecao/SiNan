# ğŸ““ Jupyter Notebook ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ `training.ipynb` è¿›è¡Œäº¤äº’å¼æ¨¡å‹å¾®è°ƒã€‚

## ğŸ“‹ ç›®å½•

- [å¯åŠ¨ Notebook](#å¯åŠ¨-notebook)
- [ä½¿ç”¨æµç¨‹](#ä½¿ç”¨æµç¨‹)
- [åŠŸèƒ½è¯¦è§£](#åŠŸèƒ½è¯¦è§£)
- [ä½¿ç”¨æŠ€å·§](#ä½¿ç”¨æŠ€å·§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)

## å¯åŠ¨ Notebook

### 1. å®‰è£…ä¾èµ–

ç¡®ä¿å·²å®‰è£… Jupyter å’Œç›¸å…³ä¾èµ–ï¼š

```bash
# å®‰è£… Jupyter
pip install jupyter notebook ipywidgets matplotlib pandas

# æˆ–ä½¿ç”¨é¡¹ç›®ä¾èµ–
pip install -r requirements.txt
```

### 2. å¯åŠ¨ Jupyter Server

```bash
cd /workspace/repos/SiNan

# å¯åŠ¨ Jupyter Notebook
jupyter notebook notebooks/

# æˆ–å¯åŠ¨ Jupyter Labï¼ˆæ¨èï¼‰
jupyter lab notebooks/
```

### 3. è®¿é—® Notebook

- æµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ `http://localhost:8888`
- ç‚¹å‡» `training.ipynb` æ‰“å¼€

## ä½¿ç”¨æµç¨‹

### Step 1: ç¯å¢ƒåˆå§‹åŒ– (Cell 1-2)

è¿è¡Œå‰ä¸¤ä¸ª Cell æ£€æŸ¥ç¯å¢ƒï¼š

```python
# Cell 1 ä¼šæ˜¾ç¤ºï¼š
# ğŸ”¥ PyTorch ç‰ˆæœ¬: 2.x.x
# ğŸ® CUDA å¯ç”¨: True
# ğŸ“º GPU: NVIDIA RTX 4090
# ğŸ’¾ GPU æ˜¾å­˜: 24.00 GB
```

**âš ï¸ æ³¨æ„**ï¼šå¦‚æœæ²¡æœ‰ GPUï¼Œè®­ç»ƒä¼šéå¸¸æ…¢ï¼

### Step 2: é…ç½®å‚æ•° (Cell 3)

ä½¿ç”¨äº¤äº’å¼æ§ä»¶é…ç½®è®­ç»ƒå‚æ•°ï¼š

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| æ¨¡å‹åç§° | HuggingFace æ¨¡å‹ ID | `google/functiongemma-270m-it` |
| æœ€å¤§åºåˆ—é•¿åº¦ | è¾“å…¥æ–‡æœ¬æœ€å¤§é•¿åº¦ | 2048 |
| LoRA Rank | ä½ç§©é€‚åº”ç»´åº¦ | 16 |
| LoRA Alpha | LoRA ç¼©æ”¾å› å­ | 16 |
| è®­ç»ƒè½®æ•° | Epoch æ•°é‡ | 3-5 |
| Batch Size | æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å° | 4-8 |
| å­¦ä¹ ç‡ | ä¼˜åŒ–å™¨å­¦ä¹ ç‡ | 2e-4 |
| æ¢¯åº¦ç´¯ç§¯ | ç´¯ç§¯æ­¥æ•° | 4 |

**ğŸ’¡ æŠ€å·§**ï¼š
- å°æ•°æ®é›† (<1000): `rank=8`, `epochs=5-10`, `lr=1e-4`
- ä¸­ç­‰æ•°æ®é›† (1000-10000): `rank=16`, `epochs=3-5`, `lr=2e-4`
- å¤§æ•°æ®é›† (>10000): `rank=32`, `epochs=2-3`, `lr=5e-5`

### Step 3: åŠ è½½æ•°æ® (Cell 4)

#### ä½¿ç”¨ç°æœ‰æ•°æ®

ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼š
```python
# é»˜è®¤è·¯å¾„
data/processed/train.jsonl
```

#### åˆ›å»ºç¤ºä¾‹æ•°æ®

å¦‚æœæ²¡æœ‰æ•°æ®ï¼ŒCell ä¼šè‡ªåŠ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ã€‚

#### æ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®åº”ä¸º JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼š

```json
{"text": "<<start_of_turn>>user\næŸ¥è¯¢åŒ—äº¬å¤©æ°”<<end_of_turn>>\n<<start_of_turn>>model\n<<start_function_call>>call:get_weather{...}<<end_function_call>><<end_of_turn>>"}
```

### Step 4: æ•°æ®åˆ†æ (Cell 5)

å¯è§†åŒ–æ•°æ®åˆ†å¸ƒï¼š
- æ–‡æœ¬é•¿åº¦ç›´æ–¹å›¾
- å·¥å…·ç±»åˆ«åˆ†å¸ƒ
- æ ·æœ¬é¢„è§ˆ

**ğŸ’¡ æŠ€å·§**ï¼šè§‚å¯Ÿæ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼Œç¡®ä¿å¤§éƒ¨åˆ†æ ·æœ¬åœ¨ `max_seq_length` èŒƒå›´å†…ã€‚

### Step 5: å¼€å§‹è®­ç»ƒ (Cell 6-7)

#### è‡ªå®šä¹‰å›è°ƒ

Notebook åŒ…å« `JupyterVisualizationCallback` ç±»ï¼Œæä¾›ï¼š
- å®æ—¶ Loss æ›²çº¿
- å­¦ä¹ ç‡å˜åŒ–
- æ¢¯åº¦èŒƒæ•°
- Epoch è¿›åº¦

#### è®­ç»ƒè¿‡ç¨‹

è¿è¡Œ Cell 7 å¼€å§‹è®­ç»ƒï¼Œä½ ä¼šçœ‹åˆ°ï¼š

```
ğŸš€ è®­ç»ƒå¼€å§‹ï¼
ğŸ“¥ åŠ è½½æ¨¡å‹...
âœ… æ¨¡å‹åŠ è½½å®Œæˆ
ğŸ¯ å¼€å§‹è®­ç»ƒ...
```

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå®æ—¶æ˜¾ç¤ºå›¾è¡¨ï¼

#### ç›‘æ§æŒ‡æ ‡

å›¾è¡¨æ¯ 5 æ­¥æ›´æ–°ä¸€æ¬¡ï¼Œæ˜¾ç¤ºï¼š
1. **Training Loss** - è®­ç»ƒæŸå¤±æ›²çº¿
2. **Learning Rate** - å­¦ä¹ ç‡è°ƒåº¦
3. **Gradient Norm** - æ¢¯åº¦èŒƒæ•°
4. **Training Progress** - Epoch è¿›åº¦

### Step 6: ä¿å­˜æ¨¡å‹ (Cell 8)

è®­ç»ƒå®Œæˆåè‡ªåŠ¨ä¿å­˜ï¼š
- æ¨¡å‹æƒé‡
- Tokenizer
- é…ç½®æ–‡ä»¶
- è®­ç»ƒæŒ‡æ ‡

### Step 7: æ¨ç†æµ‹è¯• (Cell 9-10)

#### äº¤äº’å¼æ¨ç†

ä½¿ç”¨äº¤äº’å¼ç•Œé¢æµ‹è¯•æ¨¡å‹ï¼š
- è¾“å…¥æç¤ºæ–‡æœ¬
- ç‚¹å‡»"è¿è¡Œæ¨ç†"
- æŸ¥çœ‹ç”Ÿæˆç»“æœ

#### æ‰¹é‡æµ‹è¯•

æ‰¹é‡æµ‹è¯•é¢„å®šä¹‰æç¤ºï¼š
```python
test_prompts = [
    "æŸ¥è¯¢åŒ—äº¬å¤©æ°”",
    "æŠŠèƒŒæ™¯æ”¹æˆè“è‰²",
    "åˆ›å»ºä¸€ä¸ªåå­—å«å¼ ä¸‰çš„ç”¨æˆ·",
]
```

### Step 8: å¯¼å‡ºæ¨¡å‹ (Cell 11)

æ”¯æŒä¸¤ç§å¯¼å‡ºæ ¼å¼ï¼š

#### PyTorch æ ¼å¼
- å®Œæ•´çš„ PyTorch æ¨¡å‹
- é€‚åˆè¿›ä¸€æ­¥å¾®è°ƒ
- æ–‡ä»¶è¾ƒå¤§

#### GGUF æ ¼å¼
- é‡åŒ–æ¨¡å‹
- é€‚åˆéƒ¨ç½²åˆ° Ollama/llama.cpp
- æ–‡ä»¶è¾ƒå°

**ğŸ’¡ æŠ€å·§**ï¼šç”Ÿäº§ç¯å¢ƒæ¨èä½¿ç”¨ `q8_0` é‡åŒ–ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ã€‚

## åŠŸèƒ½è¯¦è§£

### 1. å®æ—¶å¯è§†åŒ–å›è°ƒ

```python
class JupyterVisualizationCallback(TrainerCallback):
    def __init__(self, update_steps: int = 10):
        # update_steps: æ¯ N æ­¥æ›´æ–°ä¸€æ¬¡å›¾è¡¨
```

**è‡ªå®šä¹‰æ›´æ–°é¢‘ç‡**ï¼š
```python
# æ¯ 1 æ­¥æ›´æ–°ï¼ˆæ›´æµç•…ä½†æ›´è€—èµ„æºï¼‰
callbacks = [JupyterVisualizationCallback(update_steps=1)]

# æ¯ 20 æ­¥æ›´æ–°ï¼ˆæ›´èŠ‚çœèµ„æºï¼‰
callbacks = [JupyterVisualizationCallback(update_steps=20)]
```

### 2. äº¤äº’å¼æ§ä»¶

ä½¿ç”¨ ipywidgets åˆ›å»ºï¼š

```python
# æ»‘å—
widgets.IntSlider(value=16, min=4, max=64, description='LoRA Rank:')

# å¯¹æ•°æ»‘å—ï¼ˆé€‚åˆå­¦ä¹ ç‡ï¼‰
widgets.FloatLogSlider(value=2e-4, min=-5, max=-3, description='LR:')

# æ–‡æœ¬è¾“å…¥
widgets.Text(value='model_name', description='æ¨¡å‹:')

# ä¸‹æ‹‰é€‰æ‹©
widgets.Dropdown(options=['pytorch', 'gguf'], description='æ ¼å¼:')
```

### 3. æ•°æ®å¯è§†åŒ–

```python
# æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 4))
text_lengths = df['text'].str.len()
axes[0].hist(text_lengths, bins=30)

# å·¥å…·åˆ†å¸ƒ
tool_counts = df['tool_name'].value_counts()
tool_counts.plot(kind='bar', ax=axes[1])
```

## ä½¿ç”¨æŠ€å·§

### ğŸ’¡ æŠ€å·§ 1: æ–­ç‚¹ç»­è®­

å¦‚æœæƒ³ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ï¼š

```python
# ä¿®æ”¹è¾“å‡ºç›®å½•ä¸ºä¸Šæ¬¡çš„è·¯å¾„
config_widgets['output_dir'].value = 'outputs/models/experiment_20240115_120000'

# åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ
trainer.load_model()  # ä¼šè‡ªåŠ¨åŠ è½½ output_dir ä¸­çš„æ¨¡å‹
```

### ğŸ’¡ æŠ€å·§ 2: å¤šç»„å®éªŒå¯¹æ¯”

åˆ›å»ºå¤šä¸ª Notebook å®ä¾‹ï¼Œä½¿ç”¨ä¸åŒå‚æ•°ï¼š

| å®éªŒ | LoRA Rank | Learning Rate | Batch Size |
|------|-----------|---------------|------------|
| Exp 1 | 8 | 1e-4 | 8 |
| Exp 2 | 16 | 2e-4 | 4 |
| Exp 3 | 32 | 5e-5 | 2 |

### ğŸ’¡ æŠ€å·§ 3: æ˜¾å­˜ä¼˜åŒ–

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š

```python
# å‡å° batch size
config_widgets['batch_size'].value = 2

# å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼ˆä¿æŒç­‰æ•ˆ batch sizeï¼‰
config_widgets['gradient_accumulation'].value = 8

# å‡å°åºåˆ—é•¿åº¦
config_widgets['max_seq_length'].value = 1024

# é™ä½ LoRA rank
config_widgets['lora_rank'].value = 8
```

### ğŸ’¡ æŠ€å·§ 4: å¿«é€ŸéªŒè¯

åœ¨å®Œæ•´è®­ç»ƒå‰å…ˆå¿«é€ŸéªŒè¯ï¼š

```python
# ä½¿ç”¨å°æ•°æ®é›†å¿«é€Ÿæµ‹è¯•
config_widgets['epochs'].value = 1
config_widgets['data_path'].value = 'data/processed/tiny_sample.jsonl'

# å¿«é€ŸéªŒè¯æ•°æ®æ ¼å¼å’Œä»£ç 
```

### ğŸ’¡ æŠ€å·§ 5: è‡ªå®šä¹‰å›è°ƒ

æ·»åŠ è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼š

```python
class MyCustomCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 100 == 0:
            print(f"Step {state.global_step}: è‡ªå®šä¹‰å¤„ç†")

# æ·»åŠ åˆ°å›è°ƒåˆ—è¡¨
callbacks = [
    JupyterVisualizationCallback(),
    MyCustomCallback()
]
```

### ğŸ’¡ æŠ€å·§ 6: è®­ç»ƒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹

```python
from transformers import EarlyStoppingCallback

# æ·»åŠ æ—©åœå›è°ƒ
callbacks.append(
    EarlyStoppingCallback(early_stopping_patience=3)
)
```

### ğŸ’¡ æŠ€å·§ 7: è®­ç»ƒååˆ†æ

```python
# åŠ è½½è®­ç»ƒæŒ‡æ ‡
import json
with open(f"{output_dir}/training_metrics.json") as f:
    metrics = json.load(f)

# ç»˜åˆ¶å®Œæ•´æ›²çº¿
plt.plot(metrics['loss_history'])
plt.title("Training Loss")
plt.show()
```

## å¸¸è§é—®é¢˜

### Q1: å›¾è¡¨ä¸æ›´æ–°æ€ä¹ˆåŠï¼Ÿ

**A**: 
1. ç¡®ä¿å·²å¯ç”¨ matplotlib äº¤äº’æ¨¡å¼ï¼š`plt.ion()`
2. æ£€æŸ¥æ˜¯å¦åœ¨ Jupyter Lab è€Œéæ™®é€š Notebook
3. å°è¯•é‡å¯ Kernel

### Q2: äº¤äº’å¼æ§ä»¶ä¸æ˜¾ç¤ºï¼Ÿ

**A**:
```bash
# å®‰è£…å¹¶å¯ç”¨ ipywidgets
pip install ipywidgets
jupyter nbextension enable --py widgetsnbextension

# æˆ–åœ¨ Jupyter Lab
jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

### Q3: è®­ç»ƒæ—¶ Kernel å´©æºƒï¼Ÿ

**A**:
- æ£€æŸ¥ GPU æ˜¾å­˜æ˜¯å¦è€—å°½
- å‡å° batch size
- å‡å° max_seq_length
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š`use_gradient_checkpointing: true`

### Q4: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒå†å²ï¼Ÿ

**A**: è®­ç»ƒå†å²ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `metrics_history` å­—å…¸ï¼š

```python
# åœ¨æœ€åä¸€ä¸ª Cell ä¸­æŸ¥çœ‹
from collections import defaultdict
print(callback.metrics_history.keys())
# dict_keys(['loss', 'learning_rate', 'grad_norm', 'epoch'])
```

### Q5: å¦‚ä½•ä¿å­˜é«˜è´¨é‡å›¾è¡¨ï¼Ÿ

**A**: ä¿®æ”¹ä¿å­˜å‚æ•°ï¼š

```python
# åœ¨ JupyterVisualizationCallback ä¸­
plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')
```

## é«˜çº§ç”¨æ³•

### 1. å¤š GPU è®­ç»ƒ

```python
# è‡ªåŠ¨æ£€æµ‹å¤š GPU
import torch
print(f"GPU æ•°é‡: {torch.cuda.device_count()}")

# ä½¿ç”¨ DataParallelï¼ˆåœ¨é…ç½®ä¸­ï¼‰
training:
  per_device_train_batch_size: 4  # æ¯ä¸ª GPU çš„ batch size
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# åœ¨é…ç½®ä¸­é€‰æ‹©ç²¾åº¦
config_dict = {
    'model': {
        'dtype': 'bfloat16',  # æˆ– 'float16', 'float32'
    }
}
```

**æ¨è**ï¼š
- Ampere æ¶æ„ GPU (RTX 30xx/40xx, A100): ä½¿ç”¨ `bfloat16`
- æ—§æ¶æ„ GPU: ä½¿ç”¨ `float16`

### 3. è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

```python
# å¯é€‰è°ƒåº¦å™¨ç±»å‹
lr_scheduler_type_options = [
    'linear',      # çº¿æ€§è¡°å‡
    'cosine',      # ä½™å¼¦é€€ç«ï¼ˆæ¨èï¼‰
    'cosine_with_restarts',  # å¸¦é‡å¯çš„ä½™å¼¦
    'polynomial',  # å¤šé¡¹å¼è¡°å‡
    'constant',    # å¸¸æ•°
    'constant_with_warmup',  # å¸¸æ•°+é¢„çƒ­
]
```

### 4. å†»ç»“ç‰¹å®šå±‚

```python
# åœ¨åŠ è½½æ¨¡å‹åï¼Œå†»ç»“åº•å±‚
for name, param in trainer.model.named_parameters():
    if 'embed' in name or 'lm_head' in name:
        param.requires_grad = False
```

### 5. ä½¿ç”¨ Weights & Biases

```python
# å¯ç”¨ WandB
config_dict['logging']['wandb']['enabled'] = True
config_dict['logging']['wandb']['project'] = 'my-project'
config_dict['logging']['wandb']['name'] = 'experiment-1'
```

### 6. é›†æˆ TensorBoard

```python
# åœ¨ TrainingArguments ä¸­æ·»åŠ 
from transformers import TrainingArguments

training_args = TrainingArguments(
    # ... å…¶ä»–å‚æ•°
    report_to=["tensorboard"],
    logging_dir="./logs",
)
```

ç„¶ååœ¨ç»ˆç«¯è¿è¡Œï¼š
```bash
tensorboard --logdir=./logs
```

### 7. è‡ªåŠ¨åŒ–è¶…å‚æœç´¢

```python
# ä½¿ç”¨ç®€å•å¾ªç¯æµ‹è¯•å¤šç»„å‚æ•°
learning_rates = [1e-4, 2e-4, 5e-4]
ranks = [8, 16, 32]

for lr in learning_rates:
    for rank in ranks:
        config_widgets['learning_rate'].value = lr
        config_widgets['lora_rank'].value = rank
        # ... è¿è¡Œè®­ç»ƒ
```

## æœ€ä½³å®è·µ

### âœ… åº”è¯¥åšçš„

1. **å§‹ç»ˆæ£€æŸ¥ GPU çŠ¶æ€** - ç¡®ä¿ CUDA å¯ç”¨ä¸”æ˜¾å­˜å……è¶³
2. **ä»å°å‚æ•°å¼€å§‹** - å…ˆå¿«é€ŸéªŒè¯ï¼Œå†æ”¾å¤§è®­ç»ƒ
3. **ç›‘æ§æ˜¾å­˜ä½¿ç”¨** - ä½¿ç”¨ `nvidia-smi` ç›‘æ§
4. **ä¿å­˜è®­ç»ƒæ—¥å¿—** - ä¾¿äºåç»­åˆ†æå’Œå¤ç°
5. **ä½¿ç”¨éªŒè¯é›†** - é˜²æ­¢è¿‡æ‹Ÿåˆ
6. **ç‰ˆæœ¬æ§åˆ¶é…ç½®** - è®°å½•æ¯æ¬¡å®éªŒçš„å‚æ•°

### âŒ ä¸åº”è¯¥åšçš„

1. **ä¸è¦ä¸€å¼€å§‹å°±ç”¨å¤§ batch size** - å®¹æ˜“å¯¼è‡´ OOM
2. **ä¸è¦å¿½è§†æ•°æ®è´¨é‡** - åƒåœ¾è¿›åƒåœ¾å‡º
3. **ä¸è¦è®­ç»ƒå¤ªä¹…** - æ³¨æ„è§‚å¯ŸéªŒè¯é›†æŒ‡æ ‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
4. **ä¸è¦é¢‘ç¹ä¿®æ”¹å¤šä¸ªå‚æ•°** - ä¸€æ¬¡åªæ”¹ä¸€ä¸ªï¼Œä¾¿äºå®šä½é—®é¢˜

## å‚è€ƒèµ„æº

- [Jupyter Notebook å®˜æ–¹æ–‡æ¡£](https://jupyter-notebook.readthedocs.io/)
- [ipywidgets æ–‡æ¡£](https://ipywidgets.readthedocs.io/)
- [matplotlib æ•™ç¨‹](https://matplotlib.org/tutorials/index.html)
- [HuggingFace Trainer æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/trainer)

---

**æç¤º**: å¦‚æœæœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿åœ¨ GitHub Issues ä¸­æé—®ï¼
