# SiNan - FunctionGemma ä¼ä¸šå¾®è°ƒæ¡†æ¶

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**SiNan** æ˜¯ä¸€ä¸ªåŸºäº Google FunctionGemma 270M å’Œ Unsloth æ¡†æ¶çš„ä¼ä¸šçº§ AI è·¯ç”±å¾®è°ƒç³»ç»Ÿã€‚é€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥ï¼Œè‡ªåŠ¨è°ƒç”¨é¢„å®šä¹‰çš„ä¼ä¸šæ–¹æ³•ï¼Œå®ç° AI åŒ–çš„æ™ºèƒ½è·¯ç”±ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **æé€Ÿè®­ç»ƒ** - åŸºäº Unslothï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 2 å€ï¼Œæ˜¾å­˜å‡å°‘ 60%
- ğŸ“Š **Excel/CSV æ”¯æŒ** - ç›´æ¥å°†ä¼ä¸šä¸šåŠ¡æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
- ğŸ¯ **FunctionGemma ä¼˜åŒ–** - ä¸“ä¸ºå‡½æ•°è°ƒç”¨ä¼˜åŒ–çš„è½»é‡çº§æ¨¡å‹ (270M)
- ğŸ”§ **é…ç½®åŒ–** - OmegaConf å®ç°"é…ç½®å³ä»£ç "
- ğŸ“ˆ **å®æ—¶ç›‘æ§** - WandB é›†æˆï¼Œå®æ—¶æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡
- ğŸ“¦ **ä¸€é”®éƒ¨ç½²** - æ”¯æŒ GGUF é‡åŒ–ï¼Œå¯éƒ¨ç½²åˆ° CPU/GPU/è¾¹ç¼˜è®¾å¤‡

## ğŸ“¦ å®‰è£…

### å¿«é€Ÿå®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/SiNan.git
cd SiNan

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
uv venv .venv
source .venv/bin/activate

# å®‰è£…ä¾èµ–
uv pip install -r requirements.txt
```

### äº‘ç«¯å®‰è£… (æ¨è)

```bash
# è¿è¡Œç¯å¢ƒåˆå§‹åŒ–è„šæœ¬
bash scripts/setup_env.sh

# æ¿€æ´»ç¯å¢ƒ
conda activate function_gemma_env
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

åˆ›å»º Excel æ–‡ä»¶ (å¦‚ `data/raw/hr_functions.xlsx`)ï¼ŒåŒ…å«ä»¥ä¸‹åˆ—ï¼š

| User Prompt | Tool Name | Tool Args |
|-------------|-----------|-----------|
| æŸ¥è¯¢åŒ—äº¬å¤©æ°” | get_weather | `{"location": "Beijing"}` |
| æŠŠèƒŒæ™¯æ”¹æˆçº¢è‰² | change_background | `{"color": "red"}` |
| åˆ›å»ºæ–°ç”¨æˆ· | create_user | `{"name": "å¼ ä¸‰", "age": 25}` |

### 2. è½¬æ¢æ•°æ®

```bash
# è½¬æ¢ Excel ä¸º JSONL
python main.py convert data/raw/hr_functions.xlsx data/processed/hr_functions.jsonl

# æ‰¹é‡è½¬æ¢æ•´ä¸ªç›®å½•
python main.py convert data/raw/ data/processed/
```

### 3. é…ç½®è®­ç»ƒ

ç¼–è¾‘ `configs/experiments/exp_hr_routing.yaml` (å¯é€‰):

```yaml
training:
  epochs: 5
  learning_rate: 1.0e-4
  per_device_train_batch_size: 8
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
# ä½¿ç”¨åŸºç¡€é…ç½®
python main.py train --data data/processed/hr_functions.jsonl --output outputs/models/hr_v1

# ä½¿ç”¨å®éªŒé…ç½®
python main.py train --data data/processed/ --experiment exp_hr_routing
```

### 5. å¯¼å‡ºæ¨¡å‹

```bash
# å¯¼å‡ºä¸º GGUF æ ¼å¼ (ç”¨äº Ollama/llama.cpp)
python main.py export outputs/models/hr_v1 outputs/models/hr_v1_gguf --format gguf --quantization q8_0

# å¯¼å‡ºä¸º PyTorch æ ¼å¼
python main.py export outputs/models/hr_v1 outputs/models/hr_v1_pt --format pytorch
```

### 6. æ¨ç†æµ‹è¯•

```bash
python main.py inference outputs/models/hr_v1 --prompt "æŸ¥è¯¢åŒ—äº¬å¤©æ°”"
```

## ğŸ“– å‘½ä»¤è¡Œæ¥å£

```bash
# æŸ¥çœ‹æ‰€æœ‰å‘½ä»¤
python main.py --help

# æ•°æ®è½¬æ¢
python main.py convert --help

# è®­ç»ƒ
python main.py train --help

# å¯¼å‡º
python main.py export --help

# æ¨ç†
python main.py inference --help
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
SiNan/
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ base_config.yaml        # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ experiments/            # å®éªŒé…ç½®
â”‚   â””â”€â”€ templates/              # æ¨¡æ¿é…ç½®
â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                    # åŸå§‹ Excel/CSV
â”‚   â””â”€â”€ processed/              # å¤„ç†åçš„ JSONL
â”œâ”€â”€ notebooks/                  # Jupyter Notebooks
â”œâ”€â”€ outputs/                    # è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ models/                 # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ scripts/                    # Shell è„šæœ¬
â”‚   â”œâ”€â”€ setup_env.sh            # ç¯å¢ƒåˆå§‹åŒ–
â”‚   â””â”€â”€ run_cloud_train.sh      # äº‘ç«¯è®­ç»ƒ
â”œâ”€â”€ src/                        # æºä»£ç 
â”‚   â”œâ”€â”€ data_engine/            # æ•°æ®å¼•æ“
â”‚   â”‚   â”œâ”€â”€ converter.py        # æ•°æ®è½¬æ¢
â”‚   â”‚   â””â”€â”€ formatter.py        # æ ¼å¼åŒ–å™¨
â”‚   â”œâ”€â”€ training/               # è®­ç»ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ trainer.py          # è®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ callbacks.py        # å›è°ƒå‡½æ•°
â”‚   â””â”€â”€ utils/                  # å·¥å…·ç±»
â”‚       â”œâ”€â”€ config_loader.py    # é…ç½®åŠ è½½
â”‚       â””â”€â”€ export.py           # æ¨¡å‹å¯¼å‡º
â”œâ”€â”€ main.py                     # CLI ä¸»å…¥å£
â”œâ”€â”€ requirements.txt            # ä¾èµ–
â””â”€â”€ README.md                   # é¡¹ç›®æ–‡æ¡£
```

## ğŸ”§ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½® (`configs/base_config.yaml`)

```yaml
model:
  name: "google/functiongemma-270m-it"
  max_seq_length: 2048
  dtype: "bfloat16"
  lora:
    rank: 16
    alpha: 16
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"

training:
  epochs: 3
  learning_rate: 2.0e-4
  per_device_train_batch_size: 4
```

### å®éªŒé…ç½® (`configs/experiments/exp_hr_routing.yaml`)

```yaml
training:
  epochs: 5
  learning_rate: 1.0e-4

logging:
  wandb:
    project: "hr-routing-experiment"
```

## ğŸ“Š è®­ç»ƒæ•°æ®æ ¼å¼

### JSONL æ ¼å¼

æ¯è¡Œä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼š

```json
{"user_content": "æŸ¥è¯¢åŒ—äº¬å¤©æ°”", "tool_name": "get_weather", "tool_arguments": {"location": "Beijing"}}
{"user_content": "æŠŠèƒŒæ™¯æ”¹æˆçº¢è‰²", "tool_name": "change_background", "tool_arguments": {"color": "red"}}
```

### FunctionGemma Token æ ¼å¼

```
<<start_of_turn>>developer
You are a model that can do function calling with the following functions
<<start_function_declaration>>declaration:get_weather{description:<<escape>>è·å–å¤©æ°”<<escape>>,parameters:{...}}<<end_function_declaration>>
<<end_of_turn>>
<<start_of_turn>>user
æŸ¥è¯¢åŒ—äº¬å¤©æ°”<<end_of_turn>>
<<start_of_turn>>model
<<start_function_call>>call:get_weather{location:<<escape>>Beijing<<escape>>}<<end_function_call>>
<<end_of_turn>>
```

## ğŸ“ˆ ç›‘æ§ä¸å¯è§†åŒ–

### WandB é›†æˆ

è®­ç»ƒè‡ªåŠ¨è®°å½•åˆ° Weights & Biases:

```bash
# ç™»å½• WandB
wandb login

# è®­ç»ƒæ—¶è‡ªåŠ¨è®°å½•
python main.py train --data data/processed/ --experiment exp_hr_routing
```

è®¿é—® https://wandb.ai æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡ã€‚

## ğŸš€ äº‘ç«¯éƒ¨ç½²

### AWS EC2

```bash
# å¯åŠ¨å®ä¾‹ (æ¨è g4dn.xlarge - T4 GPU)
aws ec2 run-instances --image-id ami-0c55b159cbfafe1f0 --instance-type g4dn.xlarge

# SSH è¿æ¥
ssh -i key.pem ec2-user@<instance-ip>

# è¿è¡Œåˆå§‹åŒ–è„šæœ¬
bash scripts/setup_env.sh

# è®­ç»ƒ
bash scripts/run_cloud_train.sh exp_hr_routing
```

### AutoDL / RunPod

```bash
# é€‰æ‹© GPU å®ä¾‹ (T4/L4/A10)
# ä¸Šä¼ é¡¹ç›®ä»£ç 
# è¿è¡Œåˆå§‹åŒ–è„šæœ¬
bash scripts/setup_env.sh
```

## ğŸ“ æœ€ä½³å®è·µ

### æ•°æ®é‡å»ºè®®

| åœºæ™¯ | å‡½æ•°æ•° | æ¯å‡½æ•°æ ·æœ¬ | æ€»æ ·æœ¬ | é¢„æœŸå‡†ç¡®ç‡ |
|------|--------|-----------|--------|----------|
| æœ€å°å¯è¡Œ | 3-5 | 50-100 | 150-500 | ~70% |
| ç”Ÿäº§æ¨è | 10-20 | 200-500 | 2000-10000 | ~85%+ |
| é«˜è´¨é‡ | 20+ | 500+ | 10000+ | ~90%+ |

### è¶…å‚æ•°è°ƒä¼˜

```yaml
# å°æ•°æ®é›† (<1000 æ ·æœ¬)
training:
  epochs: 5-10
  learning_rate: 1.0e-4
  lora.rank: 8

# ä¸­ç­‰æ•°æ®é›† (1000-10000)
training:
  epochs: 3-5
  learning_rate: 2.0e-4
  lora.rank: 16

# å¤§æ•°æ®é›† (>10000)
training:
  epochs: 2-3
  learning_rate: 5.0e-5
  lora.rank: 32
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### CUDA Out of Memory

```yaml
# å‡å°‘ batch size
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
```

### è®­ç»ƒ Loss ä¸ä¸‹é™

- æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
- å¢åŠ å­¦ä¹ ç‡
- å¢åŠ è®­ç»ƒè½®æ•°
- æ£€æŸ¥ Token æ ¼å¼æ˜¯å¦ç¬¦åˆ FunctionGemma è§„èŒƒ

### æ¨ç†ç»“æœä¸æ­£ç¡®

- ç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡
- å¢åŠ æ¯å‡½æ•°æ ·æœ¬æ•°
- è°ƒæ•´æ¨ç†å‚æ•° (temperature, top_k, top_p)

## ğŸ“š å‚è€ƒèµ„æ–™

- [FunctionGemma å®˜æ–¹æ–‡æ¡£](https://ai.google.dev/gemma/docs/functiongemma)
- [Unsloth æ–‡æ¡£](https://unsloth.ai/docs)
- [HuggingFace TRL](https://huggingface.co/docs/trl)
- [OmegaConf](https://omegaconf.readthedocs.io/)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ“§ è”ç³»æ–¹å¼

- Email: your-email@example.com
- GitHub Issues: [æäº¤é—®é¢˜](https://github.com/your-org/SiNan/issues)
